/**
 * Test harness for attn_chunk_first_tk
 *
 * Reads test data from file, runs kernel, compares against reference.
 * Usage: ./attn_chunk_first <test_file.txt> [-v]
 */

#include <iostream>
#include <fstream>
#include <vector>
#include <cmath>
#include <cstring>
#include <sstream>
#include <iomanip>

// ============================================================================
// Configuration (must match kernel)
// ============================================================================
constexpr int MAX_N_SEQS = 32;
constexpr int CHUNK_SIZE = 64;
constexpr int D_HEAD = 128;
constexpr int BLOCK_THREADS = 64;  // 2 warps Ã— 32
constexpr int ROWS_PER_WARP = 16;  // MAX_N_SEQS / 2

// ============================================================================
// Utilities
// ============================================================================
#define CUDA_CHECK() do { \
    cudaError_t e = cudaGetLastError(); \
    if (e != cudaSuccess) { \
        fprintf(stderr, "CUDA error %s:%d: %s\n", __FILE__, __LINE__, cudaGetErrorString(e)); \
        exit(1); \
    } \
    cudaDeviceSynchronize(); \
} while(0)

void to_bf16(const float* src, bf16* dst, int n) {
    for (int i = 0; i < n; i++) dst[i] = __float2bfloat16(src[i]);
}

// ============================================================================
// Test verification
// ============================================================================
struct TestConfig {
    int n_seqs, chunk_size, d_head, n_heads, n_chunks;
};

struct Stats {
    int fails = 0, nans = 0, total = 0;
    float max_abs = 0, sum_abs = 0;
    float max_rel = 0, sum_rel = 0;

    void add(float actual, float expected, float tol) {
        float abs_diff = std::abs(actual - expected);
        float rel_diff = (expected != 0) ? 100.0f * abs_diff / std::abs(expected) : 0;

        total++;
        max_abs = std::max(max_abs, abs_diff);
        sum_abs += abs_diff;
        max_rel = std::max(max_rel, rel_diff);
        sum_rel += rel_diff;

        if (std::isnan(actual) || std::isinf(actual)) nans++;
        else if (abs_diff > tol) fails++;
    }

    float avg_abs() const { return total > 0 ? sum_abs / total : 0; }
    float avg_rel() const { return total > 0 ? sum_rel / total : 0; }
    bool pass() const { return fails == 0 && nans == 0; }
};

bool verify(const TestConfig& cfg,
            const float* actual_attns, const float* actual_maxs, const float* actual_sums,
            const std::vector<std::vector<float>>& ref_attns,
            const std::vector<std::vector<float>>& ref_maxs,
            const std::vector<std::vector<float>>& ref_sums,
            const std::vector<int>& offsets,
            bool verbose) {

    Stats attn, maxs, sums;
    int first_errors = 0;

    for (int c = 0; c < cfg.n_chunks; c++) {
        int off = offsets[c];

        // Check attention outputs
        for (int h = 0; h < cfg.n_heads; h++) {
            for (int s = 0; s < cfg.n_seqs; s++) {
                for (int d = 0; d < cfg.d_head; d++) {
                    int local = h * cfg.n_seqs * cfg.d_head + s * cfg.d_head + d;
                    int global = off * cfg.n_heads * cfg.d_head + local;

                    float act = actual_attns[global];
                    float exp = ref_attns[c][local];
                    float tol = std::max(0.1f, std::abs(exp) * 0.03f);

                    int prev_fails = attn.fails + attn.nans;
                    attn.add(act, exp, tol);

                    if (verbose && (attn.fails + attn.nans) > prev_fails && first_errors < 5) {
                        std::cout << "  FAIL attn[c=" << c << " h=" << h << " s=" << s << " d=" << d
                                  << "] act=" << act << " exp=" << exp << " diff=" << std::abs(act-exp) << "\n";
                        first_errors++;
                    }
                }
            }
        }

        // Check max/sum
        for (int h = 0; h < cfg.n_heads; h++) {
            for (int s = 0; s < cfg.n_seqs; s++) {
                int local = h * cfg.n_seqs + s;
                int global = off * cfg.n_heads + local;

                float max_act = actual_maxs[global], max_exp = ref_maxs[c][local];
                float sum_act = actual_sums[global], sum_exp = ref_sums[c][local];

                maxs.add(max_act, max_exp, std::max(0.1f, std::abs(max_exp) * 0.03f));
                sums.add(sum_act, sum_exp, std::max(0.1f, std::abs(sum_exp) * 0.03f));
            }
        }
    }

    bool all_pass = attn.pass() && maxs.pass() && sums.pass();

    // Print results
    if (verbose || !all_pass) {
        std::cout << "\n=== Results ===\n";
        std::cout << "Attention: " << (attn.pass() ? "PASS" : "FAIL")
                  << " (fails=" << attn.fails << "/" << attn.total << " nan=" << attn.nans << ")\n"
                  << "  Abs: max=" << attn.max_abs << " avg=" << attn.avg_abs() << "\n"
                  << "  Rel: max=" << std::fixed << std::setprecision(2) << attn.max_rel
                  << "% avg=" << attn.avg_rel() << "%\n";
        std::cout << "Max values: " << (maxs.pass() ? "PASS" : "FAIL")
                  << " (fails=" << maxs.fails << "/" << maxs.total << " nan=" << maxs.nans << ")\n"
                  << "  Abs: max=" << maxs.max_abs << " avg=" << maxs.avg_abs() << "\n";
        std::cout << "Sum values: " << (sums.pass() ? "PASS" : "FAIL")
                  << " (fails=" << sums.fails << "/" << sums.total << " nan=" << sums.nans << ")\n"
                  << "  Abs: max=" << sums.max_abs << " avg=" << sums.avg_abs() << "\n";
    }

    std::cout << (all_pass ? "PASSED" : "FAILED") << "\n";

    // Summary line for test runner
    int total_fails = attn.fails + attn.nans;
    float fail_rate = 100.0f * total_fails / attn.total;
    std::cout << "SUMMARY: fails=" << total_fails << "/" << attn.total
              << " (" << std::fixed << std::setprecision(2) << fail_rate << "%)"
              << " max_diff=" << attn.max_abs << "\n";

    return all_pass;
}

// ============================================================================
// Main
// ============================================================================
int main(int argc, char** argv) {
    if (argc < 2) {
        std::cerr << "Usage: " << argv[0] << " <test.txt> [-v]\n";
        return 1;
    }

    bool verbose = (argc > 2 && strcmp(argv[2], "-v") == 0);

    // Read test file
    std::ifstream f(argv[1]);
    if (!f) { std::cerr << "Cannot open " << argv[1] << "\n"; return 1; }

    TestConfig cfg;
    f >> cfg.n_seqs >> cfg.chunk_size >> cfg.d_head >> cfg.n_heads >> cfg.n_chunks;

    if (cfg.n_seqs > MAX_N_SEQS || cfg.chunk_size != CHUNK_SIZE || cfg.d_head != D_HEAD) {
        std::cerr << "Config mismatch: n_seqs=" << cfg.n_seqs << " (max " << MAX_N_SEQS << "), "
                  << "chunk_size=" << cfg.chunk_size << " (need " << CHUNK_SIZE << "), "
                  << "d_head=" << cfg.d_head << " (need " << D_HEAD << ")\n";
        return 1;
    }

    if (verbose) {
        std::cout << "Config: n_seqs=" << cfg.n_seqs << " n_heads=" << cfg.n_heads
                  << " n_chunks=" << cfg.n_chunks << "\n";
    }

    // Sizes
    const int q_size = cfg.n_seqs * cfg.n_heads * cfg.d_head;
    const int kv_size = cfg.n_heads * cfg.chunk_size * cfg.d_head;
    const int out_size = cfg.n_heads * cfg.n_seqs * cfg.d_head;
    const int ms_size = cfg.n_heads * cfg.n_seqs;

    // Read data
    std::vector<float> h_q(q_size);
    std::vector<std::vector<float>> h_k(cfg.n_chunks), h_v(cfg.n_chunks);
    std::vector<std::vector<float>> ref_out(cfg.n_chunks), ref_max(cfg.n_chunks), ref_sum(cfg.n_chunks);

    for (auto& x : h_q) f >> x;
    for (int c = 0; c < cfg.n_chunks; c++) { h_k[c].resize(kv_size); for (auto& x : h_k[c]) f >> x; }
    for (int c = 0; c < cfg.n_chunks; c++) { h_v[c].resize(kv_size); for (auto& x : h_v[c]) f >> x; }
    for (int c = 0; c < cfg.n_chunks; c++) { ref_out[c].resize(out_size); for (auto& x : ref_out[c]) f >> x; }
    for (int c = 0; c < cfg.n_chunks; c++) { ref_max[c].resize(ms_size); for (auto& x : ref_max[c]) f >> x; }
    for (int c = 0; c < cfg.n_chunks; c++) { ref_sum[c].resize(ms_size); for (auto& x : ref_sum[c]) f >> x; }

    // Allocate device memory
    bf16 *d_q;
    std::vector<bf16*> d_k(cfg.n_chunks), d_v(cfg.n_chunks);
    float *d_attns, *d_maxs, *d_sums;
    void **d_keys, **d_vals;
    int *d_offsets, *d_begins, *d_ends;

    cudaMalloc(&d_q, q_size * sizeof(bf16));
    for (int c = 0; c < cfg.n_chunks; c++) {
        cudaMalloc(&d_k[c], kv_size * sizeof(bf16));
        cudaMalloc(&d_v[c], kv_size * sizeof(bf16));
    }

    int total_out = cfg.n_chunks * out_size;
    int total_ms = cfg.n_chunks * ms_size;
    cudaMalloc(&d_attns, total_out * sizeof(float));
    cudaMalloc(&d_maxs, total_ms * sizeof(float));
    cudaMalloc(&d_sums, total_ms * sizeof(float));
    cudaMemset(d_attns, 0, total_out * sizeof(float));
    cudaMemset(d_maxs, 0, total_ms * sizeof(float));
    cudaMemset(d_sums, 0, total_ms * sizeof(float));

    // Convert to bf16 and copy
    std::vector<bf16> tmp(std::max(q_size, kv_size));
    to_bf16(h_q.data(), tmp.data(), q_size);
    cudaMemcpy(d_q, tmp.data(), q_size * sizeof(bf16), cudaMemcpyHostToDevice);

    for (int c = 0; c < cfg.n_chunks; c++) {
        to_bf16(h_k[c].data(), tmp.data(), kv_size);
        cudaMemcpy(d_k[c], tmp.data(), kv_size * sizeof(bf16), cudaMemcpyHostToDevice);
        to_bf16(h_v[c].data(), tmp.data(), kv_size);
        cudaMemcpy(d_v[c], tmp.data(), kv_size * sizeof(bf16), cudaMemcpyHostToDevice);
    }

    // Setup pointer arrays
    std::vector<void*> h_keys(cfg.n_chunks), h_vals(cfg.n_chunks);
    for (int c = 0; c < cfg.n_chunks; c++) { h_keys[c] = d_k[c]; h_vals[c] = d_v[c]; }
    cudaMalloc(&d_keys, cfg.n_chunks * sizeof(void*));
    cudaMalloc(&d_vals, cfg.n_chunks * sizeof(void*));
    cudaMemcpy(d_keys, h_keys.data(), cfg.n_chunks * sizeof(void*), cudaMemcpyHostToDevice);
    cudaMemcpy(d_vals, h_vals.data(), cfg.n_chunks * sizeof(void*), cudaMemcpyHostToDevice);

    // Setup metadata (each chunk covers all seqs)
    std::vector<int> h_begins(cfg.n_chunks, 0);
    std::vector<int> h_ends(cfg.n_chunks, cfg.n_seqs);
    std::vector<int> h_offsets(cfg.n_chunks);
    for (int c = 0; c < cfg.n_chunks; c++) h_offsets[c] = c * cfg.n_seqs;

    cudaMalloc(&d_begins, cfg.n_chunks * sizeof(int));
    cudaMalloc(&d_ends, cfg.n_chunks * sizeof(int));
    cudaMalloc(&d_offsets, cfg.n_chunks * sizeof(int));
    cudaMemcpy(d_begins, h_begins.data(), cfg.n_chunks * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_ends, h_ends.data(), cfg.n_chunks * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_offsets, h_offsets.data(), cfg.n_chunks * sizeof(int), cudaMemcpyHostToDevice);
    CUDA_CHECK();

    // Setup kernel globals
    using Globals = attn_chunk_first_globals<MAX_N_SEQS, D_HEAD>;
    Globals g {
        .attns = gl<float, -1, -1, -1, -1>(d_attns, 1, 1, total_out / D_HEAD, D_HEAD),
        .maxs = gl<float, -1, -1, -1, -1>(d_maxs, 1, 1, 1, total_ms),
        .sums = gl<float, -1, -1, -1, -1>(d_sums, 1, 1, 1, total_ms),
        .offsets = gl<int, -1, -1, -1, -1>(d_offsets, 1, 1, 1, cfg.n_chunks),
        .begins = gl<int, -1, -1, -1, -1>(d_begins, 1, 1, 1, cfg.n_chunks),
        .ends = gl<int, -1, -1, -1, -1>(d_ends, 1, 1, 1, cfg.n_chunks),
        .Q = d_q,
        .keys = d_keys,
        .values = d_vals,
        .scale = 1.0f / sqrtf(float(cfg.d_head)),
        .n_heads = cfg.n_heads
    };

    // Compute shared memory size
    size_t smem = sizeof(st<bf16, CHUNK_SIZE, D_HEAD>) * 2 +      // K, V shared
                  sizeof(st<bf16, ROWS_PER_WARP, D_HEAD>) * 2 +   // Q per-warp
                  sizeof(st<float, ROWS_PER_WARP, CHUNK_SIZE>) * 2 + // scores per-warp
                  sizeof(st<float, ROWS_PER_WARP, D_HEAD>) * 2;   // output per-warp
    smem = ((smem + 4095) / 4096) * 4096;

    cudaFuncSetAttribute(attn_chunk_first_tk<MAX_N_SEQS, CHUNK_SIZE, D_HEAD>,
                         cudaFuncAttributeMaxDynamicSharedMemorySize, smem);
    CUDA_CHECK();

    // Launch
    dim3 grid(cfg.n_heads, cfg.n_chunks);
    attn_chunk_first_tk<MAX_N_SEQS, CHUNK_SIZE, D_HEAD><<<grid, BLOCK_THREADS, smem>>>(g);
    CUDA_CHECK();

    // Copy results back
    std::vector<float> h_attns(total_out), h_maxs(total_ms), h_sums(total_ms);
    cudaMemcpy(h_attns.data(), d_attns, total_out * sizeof(float), cudaMemcpyDeviceToHost);
    cudaMemcpy(h_maxs.data(), d_maxs, total_ms * sizeof(float), cudaMemcpyDeviceToHost);
    cudaMemcpy(h_sums.data(), d_sums, total_ms * sizeof(float), cudaMemcpyDeviceToHost);

    // Verify
    bool pass = verify(cfg, h_attns.data(), h_maxs.data(), h_sums.data(),
                       ref_out, ref_max, ref_sum, h_offsets, verbose);

    // Cleanup
    cudaFree(d_q);
    for (int c = 0; c < cfg.n_chunks; c++) { cudaFree(d_k[c]); cudaFree(d_v[c]); }
    cudaFree(d_attns); cudaFree(d_maxs); cudaFree(d_sums);
    cudaFree(d_keys); cudaFree(d_vals);
    cudaFree(d_offsets); cudaFree(d_begins); cudaFree(d_ends);

    return pass ? 0 : 1;
}
